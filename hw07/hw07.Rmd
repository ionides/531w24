---
title: "Homework 7"
author: "DATASCI/STATS 531, Liangqi Tang"
format: 
  html:
    toc: TRUE
    toc-depth: 5
    code-fold: TRUE
    code-summary: "Code"
    code-tools: TRUE
    embed-resources: TRUE
---

\newcommand\prob{\mathbb{P}}
\newcommand\E{\mathbb{E}}
\newcommand\var{\mathrm{Var}}
\newcommand\cov{\mathrm{Cov}}
\newcommand\data[1]{#1^*}

--------

Please submit your homework report to Canvas as both an Rmarkdown (Rmd) file and an html file produced by it. You are welcome to collaborate with other members of your final project group: you have a collective incentive to learn how to take advantage of greatlakes, and the tools practiced in this homework will be useful for the final project. You are also welcome to post on Piazza, either sharing advice or asking questions. You should run your own code, and as usual you should report on all sources and give proper acknowledgement of the extent of their contributions. Proper acknowledgement involves both listing sources at the end of the report and citing the sources at all appropriate points during the report. It is expected that your solution to Question 7.2 will involve borrowing code provided in the notes. Past solutions are also available, if you need extra hints, but you may learn more by starting from the code in the notes. Either way, your report should be explicit about what you borrowed and from where. Your report should document issues that arose and explain the work you put into your solution.

------------

# Question 7.1 Introduction to the greatlakes cluster

The greatlakes cluster is a collection of high-performance Linux machines operated by University of Michigan. Each machine has 36 CPU cores. This facilitates computationally intensive Monte Carlo statistical inference, allowing more thorough investigations than are possible on a laptop. Linux cluster computing is the standard platform for computationally intensive statistics and data science, so learning how to work on greatlakes is worthwhile, if this is new to you. This question may be easy if you are already familiar with greatlakes. It is possible to access Rstudio on greatlakes from a web interface. However, for larger tasks it is better to submit batch jobs, and that is that we practice here. Once you have successfully run a simple parallel R command, following the instructions below, it is fairly straightforward to run any foreach loop in parallel.

Read the [greatlakes notes on the course website](../greatlakes/index.html) and work through the example to run the parallel foreach in the file [test.R](../greatlakes/test.R) on greatlakes. 

## (a) 

Report on any issues you had to overcome to run the test code as a batch job on greatlakes. Did everything go smoothly, or were there problems you had to overcome?

**Answer**:

Generally everything goes smoothly except when I tried to install packages following the guides on the slides, I was told that the directory is not writtable so I needed to choose to create a personal directory in my space to continue. And this installing process took a lot more time compared to that in Rstudio. Besides, it took me much time to copy the remote file to local because I made the mistake that I repeatedly ran `scp tanglq@greatlakes-xfer.arc-ts.umich.edu:531w24/greatlakes/test.csv /Users/liangqi/Desktop/` on ssh session while I should ran it on my local terminal.

****

## (b) 

Have you used a Linux cluster before?

**Answer**:

I have not used a linux cluster before. I have known some basic linux commands and I have also used Great Lakes. However last time I used Great Lakes I basically use the user interface to do tasks and I never tried to use linux a terminal to operate all things.

****

## (c) 

Compare the run times reported by test.R for both greatlakes and your laptop. How do you interpret these results?

**Answer**:

In this question, I used some guidance of linux commands on slides and Great Lakes websites.

on laptop:

```{r, message = FALSE, cache = TRUE}
library(knitr)
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/531/Stats531/hw07")

time1 <- read.csv("test_local.csv", sep = " ")
kable(time1)
```

on GreatLakes:

```{r, message = FALSE, cache = TRUE}
time2 <- read.csv("test_remote.csv", sep = " ")
kable(time2)
```

It seems counter-intuitive since the test runs faster on my local machine than on Greatlakes computational clusters. I guess the reason might be that I did not set the CPUs or GPUs that I can use on the remote so the speed is limited? Another possible reason is that the test example runs too fast that we can not make the full use of Great Lakes.

****

# Question 7.2 Likelihood maximization for the SEIR model

We consider an SEIR model for the Consett measles epidemic, which is the same model and data used for Homework 6. Write a report presenting the following steps. You will need to tailor the intensity of your search to the computational resources at your disposal. In particular, choose the number of starting points, number of particles employed, and the number of IF2 iterations appropriately for the size and speed of your machine. It is okay for this homework if the Monte Carlo error is larger than you would like.

Develop your code to run in parallel on all the cores of your laptop and then run the same code on greatlakes. Report on the change in computing time.

**Preparation**:

First we need to load the data, pomp and the stochastic SEIR model we constructed in Homework 6:
(cite: [codes in HW6](https://github.com/LiangqiTang/Stats-531/blob/main/hw06/hw06.Rmd))

```{r, message = FALSE, cache = TRUE}
# load the data
library(tidyverse)
library(ggplot2)
read_csv(paste0("https://kingaa.github.io/sbied/stochsim/",
  "Measles_Consett_1948.csv")) |>
  select(week,reports=cases) -> meas

# implement the SIR model in pomp
library(pomp)

# step function: E is similar to the component I, add E in the chain
seir_step <- Csnippet("
  double dN_SE = rbinom(S,1-exp(-Beta*I/N*dt));
  double dN_EI = rbinom(E,1-exp(-mu_EI*dt));
  double dN_IR = rbinom(I,1-exp(-mu_IR*dt));
  S -= dN_SE;
  E += dN_SE - dN_EI;
  I += dN_EI - dN_IR;
  R += dN_IR;
  H += dN_IR;
")

# initial function: suppose we do not no the latent value of E at first.
seir_rinit <- Csnippet("
  S = nearbyint(eta*N);
  E = 0; 
  I = 1;
  R = nearbyint((1-eta)*N);
  H = 0;
")

# dmeasure component
seir_dmeas <- Csnippet("
  lik = dnbinom_mu(reports,k,rho*H,give_log);
")

# rmeasure component
seir_rmeas <- Csnippet("
  reports = rnbinom_mu(k,rho*H);
")

# add components to pomp model: clarify E and mu_EI
meas |>
  pomp(times="week",t0=0,
    rprocess=euler(seir_step,delta.t=1/7),
    rinit=seir_rinit,
    rmeasure=seir_rmeas,
    dmeasure=seir_dmeas,
    accumvars="H",
    statenames=c("S","E","I","R","H"),
    paramnames=c("Beta","mu_EI","mu_IR","N","eta","rho","k"),
    params=c(Beta=30,mu_EI=0.8,mu_IR=1,rho=0.5,k=10,eta=0.05,N=38000)
  ) -> measSEIR
```

Then test the codes: filtering:

```{r, message = FALSE, cache = TRUE}
measSEIR |>
  pfilter(Np = 1000) -> pf

plot(pf)
```

We can see that a single filtering can work. Now we fix N, mu_IR and k as the [slides14](https://ionides.github.io/531w24/14/slides-annotated.pdf) do.

```{r, message = FALSE, cache = TRUE, warning = FALSE}
fixed_params <- c(N=38000, mu_EI=1.5, mu_IR=2, k=10) # here I additionally fix mu_EI=1.5
coef(measSEIR, names(fixed_params)) <- fixed_params
```

Then set parallel computing.

```{r, message = FALSE, cache = TRUE}
library(foreach)
library(doFuture)
plan(multisession)
```

Now we proceed to estimate $\beta, \eta, \rho$ and mu_EI. First here we run a particle filter and store the starting point in csv:

```{r, message = FALSE, cache = TRUE}
# running a particle filter
foreach(i=1:10,.combine=c,
.options.future=list(seed=TRUE)
) %dofuture% {
measSEIR |> pfilter(Np=5000)
} -> pf
pf |> logLik() |> logmeanexp(se=TRUE) -> L_pf
L_pf
```

```{r, message = FALSE, cache = TRUE}
pf[[1]] |> coef() |> bind_rows() |>
bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) |>
write_csv("measles_params.csv")
```

****

## (a) 

Conduct a local search and then a global search using the multi-stage, multi-start method.

**Answer**:

cite: [slides14](https://ionides.github.io/531w24/14/slides-annotated.pdf)

### local search

```{r, message = FALSE, cache = TRUE}
# do local search
foreach(i=1:20,.combine=c,
.options.future=list(seed=482947940)
) %dofuture% {
measSEIR |>
mif2(
Np=2000, Nmif=50,
cooling.fraction.50=0.5,
rw.sd=rw_sd(Beta=0.02, rho=0.02, eta=ivp(0.02)),
partrans=parameter_trans(log="Beta",logit=c("rho","eta")),
paramnames=c("Beta","rho","eta")
)
} -> mifs_local
```

**Iterated filtering diagnostics**

```{r, message = FALSE, cache = TRUE}
# Iterated filtering diagnostics
mifs_local |>
traces() |>
melt() |>
ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+
geom_line()+
guides(color="none")+
facet_wrap(~name,scales="free_y")
```

**Estimating the likelihood**

```{r, message = FALSE, cache = TRUE}
# estimating the likelihood
foreach(mf=mifs_local,.combine=rbind,
.options.future=list(seed=900242057)
) %dofuture% {
evals <- replicate(10, logLik(pfilter(mf,Np=5000)))
ll <- logmeanexp(evals,se=TRUE)
mf |> coef() |> bind_rows() |>
bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results_local

pairs(~loglik+Beta+eta+rho,data=results_local,pch=16)
```

**Build up a picture of the likelihood surface**

```{r, message = FALSE, cache = TRUE}
# build up a picture of the likelihood surface
read_csv("measles_params.csv") |>
bind_rows(results_local) |>
arrange(-loglik) |>
write_csv("measles_params.csv")
```

### global search using the multi-stage, multi-start method

We use $\beta \in (5,80), \rho \in (0.2,0.9), \eta\in(0,1)$ as [slides14](https://ionides.github.io/531w24/14/slides-annotated.pdf).

```{r, message = FALSE, cache = TRUE}
set.seed(2062379496)
runif_design(
lower=c(Beta=5,rho=0.2,eta=0),
upper=c(Beta=80,rho=0.9,eta=1),
nseq=200 # change start points from 400 to 200
) -> guesses
mf1 <- mifs_local[[1]]
```

```{r, message = FALSE, cache = TRUE}
library(iterators)

foreach(guess=iter(guesses,"row"), .combine=rbind,
.options.future=list(seed=1270401374)
) %dofuture% {
mf1 |>
mif2(params=c(guess,fixed_params)) |>
mif2(Nmif=100) -> mf 
replicate(
10,
mf |> pfilter(Np=1500) |> logLik() # change Np from 5000 to 1500
) |>
logmeanexp(se=TRUE) -> ll
mf |> coef() |> bind_rows() |>
bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results_global
```

**Show scatter plot matrix**

```{r, message = FALSE, cache = TRUE}
# build up a picture of the likelihood surface
read_csv("measles_params.csv") |>
bind_rows(results_global) |>
arrange(-loglik) |>
write_csv("measles_params.csv")
```

```{r, message = FALSE, cache = TRUE}
# show scatter plot matrix
read_csv("measles_params.csv") |>
#filter(loglik>max(loglik)-50) |> # can't not filt, otherwise no data
bind_rows(guesses) |>
mutate(type=if_else(is.na(loglik),"guess","result")) |>
arrange(type) -> all

pairs(~loglik+Beta+eta+rho, data=all, pch=16, cex=0.3,
col=ifelse(all$type=="guess",grey(0.5),"red"))
```

**Poor man's profile likelihood**

```{r, message = FALSE, cache = TRUE}
# poor man's profile likelihood
all |>
filter(type=="result") |>
filter(loglik>max(loglik)-10) |>
ggplot(aes(x=eta,y=loglik))+
geom_point()+
labs(
x=expression(eta),
title="poor manâ€™s profile likelihood"
)
```

****

## (b) 

How does the maximized likelihood for the SEIR model compare with what we obtained for the SIR model?

**Answer**:

```{r, message = FALSE, cache = TRUE}
local_global <- read_csv("measles_params.csv")
local_global[which.max(local_global$loglik[!is.na(local_global$loglik)]), c("loglik","loglik.se")]
```

Compared with -104.3 we got in SIR model, SEIR model improve the best result likelihood a little.

****

## (c) 

How do the parameter estimates differ between SIR and SEIR?

**Answer**:

```{r, message = FALSE, cache = TRUE}
local_global[which.max(local_global$loglik),c("Beta","rho","eta")]
```

The result is not very close to parameter estimates in SIR. And the trend of loglikelihood against parameter is not very similar.

****

## (d) 

Calculate and plot a profile likelihood over the reporting rate for the SEIR model. Construct a 95% confidence interval for the reporting rate, and discuss how this profile compares with the SIR profile in Chapter 14.

**Answer**:

**Profile Likelihood over** $\rho$

```{r, message = FALSE, cache = TRUE}
read_csv("measles_params.csv") |>
#filter(loglik>max(loglik)-20,loglik.se<2) |>
sapply(range) -> box
box
```

```{r, message = FALSE, cache = TRUE}
freeze(seed=1196696958,
profile_design(
rho=seq(0.01,0.95,length=40),
lower=box[1,c("Beta","eta")],
upper=box[2,c("Beta","eta")],
nprof=15, type="runif"
)) -> guesses
plot(guesses)
```

```{r, message = FALSE, cache = TRUE}
foreach(guess=iter(guesses,"row"), .combine=rbind,
.options.future=list(seed=830007657)
) %dofuture% {
mf1 |>
mif2(params=c(guess,fixed_params),
rw.sd=rw_sd(Beta=0.02,eta=0.02)) |>
mif2(Nmif=30,cooling.fraction.50=0.3) -> mf # change Nmif from 100 to 30
replicate(
10,
mf |> pfilter(Np=1000) |> logLik()) |> # change Np from 5000 to 1000
logmeanexp(se=TRUE) -> ll
mf |> coef() |> bind_rows() |>
bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results_profile
```

**Visualizing Profile Likelihood**

```{r, message = FALSE, cache = TRUE}
read_csv("measles_params.csv") |>
bind_rows(results_profile) |>
filter(is.finite(loglik)) |>
arrange(-loglik) |>
write_csv("measles_params.csv")
```

```{r, message = FALSE, cache = TRUE}
read_csv("measles_params.csv") |>
filter(loglik>max(loglik)-10) -> all
pairs(~loglik+Beta+eta+rho,data=all,pch=16)
```

```{r, message = FALSE, cache = TRUE}
results_profile |>
filter(is.finite(loglik)) |>
group_by(round(rho,5)) |>
filter(rank(-loglik)<3) |>
ungroup() |>
filter(loglik>max(loglik)-20) |>
ggplot(aes(x=rho,y=loglik))+
geom_point()
```

**Confidence Interval**

```{r, message = FALSE, warning= FALSE, cache = TRUE}
maxloglik <- max(results_profile$loglik,na.rm=TRUE)
ci.cutoff <- maxloglik-0.5*qchisq(df=1,p=0.95)
results_profile |>
filter(is.finite(loglik)) |>
group_by(round(rho,5)) |>
filter(rank(-loglik)<3) |>
ungroup() |>
ggplot(aes(x=rho,y=loglik))+
geom_point()+
geom_smooth(method="loess",span=0.25)+
geom_hline(color="red",yintercept=ci.cutoff)+
lims(y=maxloglik-c(5,0))
```

compared with the profile likelihood plot on $\rho$ of SIR, the profile likelihood of SEIR has larger optimal $\rho$ and wider confidence interval in [0,1].


****

## laptop vs Greatlakes computing time

**Answer**:

Here I especially write down the time used for local search, global search and profile likelihood search.

**local laptop**:

```{r}
time3 <- read.csv("local_time.csv", sep = " ")
kable(time3)
```

**remote GreatLakes**
```{r}
time4 <- read.csv("remote_time.csv", sep = " ")
kable(time4)
```

Comparing the results we can find that Great Lakes runs much faster than local laptop on time-consuming tasks.

****

# Acknowledgements

Question 7.2 derives from material in [Simulation-based Inference for Epidemiological Dynamics](http://kingaa.github.io/sbied/index.html).

---------------

# Reference

- 1. [codes in HW6](https://github.com/LiangqiTang/Stats-531/blob/main/hw06/hw06.Rmd)
- 2. [slides14](https://ionides.github.io/531w24/14/slides-annotated.pdf)
